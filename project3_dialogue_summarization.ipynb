{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alinakhodotovych2022/Project3/blob/main/project3_dialogue_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaOs6Z276dgo"
      },
      "source": [
        "# Project 3 — Dialogue Summarization with BART-Large\n",
        "**Author: Alina K.**"
      ],
      "id": "TaOs6Z276dgo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juPMlSDe6dgq"
      },
      "source": [
        "## 1. Setup\n",
        "Install required libraries."
      ],
      "id": "juPMlSDe6dgq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6Y7xEQ76dgr",
        "outputId": "0c3e195a-6771-48fb-feaa-6e2ffe783f95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets evaluate rouge_score sentencepiece accelerate"
      ],
      "id": "E6Y7xEQ76dgr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK1_JC996dgs"
      },
      "source": [
        "## 2. Imports"
      ],
      "id": "OK1_JC996dgs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndSxm_IO6dgs"
      },
      "execution_count": 2,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "import evaluate\n"
      ],
      "id": "ndSxm_IO6dgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3N4QIl6dgt"
      },
      "source": [
        "## 3. Load SAMSum Dataset"
      ],
      "id": "Cj3N4QIl6dgt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ELU7rpMj6dgt",
        "outputId": "b41532e4-d5da-4c55-888a-1141ede4248c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'samsum' doesn't exist on the Hub or cannot be accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1948944377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'samsum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msamsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 ) from e\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 api.hf_hub_download(\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'samsum' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ],
      "source": [
        "samsum = load_dataset('samsum')\n",
        "samsum"
      ],
      "id": "ELU7rpMj6dgt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huxe_Zjv6dgu"
      },
      "source": [
        "## 4. Preprocessing & Tokenization"
      ],
      "id": "Huxe_Zjv6dgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4slcZqS6dgu"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "\n",
        "def preprocess(batch):\n",
        "    inputs = tokenizer(batch['dialogue'], max_length=512, truncation=True)\n",
        "    labels = tokenizer(batch['summary'], max_length=96, truncation=True)\n",
        "    inputs['labels'] = labels['input_ids']\n",
        "    return inputs\n",
        "\n",
        "tokenized_samsum = samsum.map(preprocess, batched=True, remove_columns=samsum['train'].column_names)\n"
      ],
      "id": "d4slcZqS6dgu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u9O2PFy6dgv"
      },
      "source": [
        "## 5. Model Initialization"
      ],
      "id": "6u9O2PFy6dgv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JmB-67S6dgv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
      ],
      "id": "_JmB-67S6dgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCEvSdrG6dgv"
      },
      "source": [
        "## 6. Training Configuration"
      ],
      "id": "BCEvSdrG6dgv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjEvcAT6dgv"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='bart-samsum',\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    lr_scheduler_type='linear',\n",
        "    learning_rate=3e-5,\n",
        "    fp16=True,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_samsum['train'],\n",
        "    eval_dataset=tokenized_samsum['validation'],\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "id": "HHjEvcAT6dgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEljQImz6dgw"
      },
      "source": [
        "## 7. Train Model"
      ],
      "id": "uEljQImz6dgw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIyRiwkD6dgw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "trainer.train()"
      ],
      "id": "FIyRiwkD6dgw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP11esDW6dgw"
      },
      "source": [
        "## 8. Evaluation — ROUGE"
      ],
      "id": "DP11esDW6dgw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIVVgr0Q6dgw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "preds = trainer.predict(tokenized_samsum['test'])\n",
        "\n",
        "decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(preds.label_ids, skip_special_tokens=True)\n",
        "\n",
        "rouge_results = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "rouge_results"
      ],
      "id": "VIVVgr0Q6dgw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9_-KzY36dgx"
      },
      "source": [
        "## 9. Example Inference"
      ],
      "id": "O9_-KzY36dgx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztkIRtSr6dgx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "example = samsum['test'][0]['dialogue']\n",
        "inputs = tokenizer(example, return_tensors='pt', truncation=True, max_length=512)\n",
        "summary_ids = model.generate(**inputs, max_length=80, num_beams=4)\n",
        "print(tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
      ],
      "id": "ztkIRtSr6dgx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OZhWaj_6dgx"
      },
      "source": [
        "## 10. Save Model"
      ],
      "id": "9OZhWaj_6dgx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLhfKbcp6dgx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.save_pretrained('bart-samsum')\n",
        "tokenizer.save_pretrained('bart-samsum')"
      ],
      "id": "KLhfKbcp6dgx"
    }
  ]
}